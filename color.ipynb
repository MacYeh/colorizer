{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hohsiny\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Flatten, Reshape, Conv2D, UpSampling2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import Callback\n",
    "import random\n",
    "import glob\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR: Headless mode isn't supported on Windows. If you want to use W&B, please use \"wandb run ...\"; running normally.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "run = wandb.init()\n",
    "config = run.config\n",
    "\n",
    "config.num_epochs = 1\n",
    "config.batch_size = 4\n",
    "config.img_dir = \"images\"\n",
    "config.height = 256\n",
    "config.width = 256\n",
    "\n",
    "val_dir = 'test'\n",
    "train_dir = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatically get the data if it doesn't exist\n",
    "if not os.path.exists(\"train\"):\n",
    "    print(\"Downloading flower dataset...\")\n",
    "    subprocess.check_output(\"curl https://storage.googleapis.com/l2kzone/flowers.tar | tar xz\", shell=True)\n",
    "\n",
    "def my_generator(batch_size, img_dir):\n",
    "    \"\"\"A generator that returns black and white images and color images\"\"\"\n",
    "    image_filenames = glob.glob(img_dir + \"\\*\")\n",
    "    counter = 0\n",
    "    while True:\n",
    "        bw_images = np.zeros((batch_size, config.width, config.height))\n",
    "        color_images = np.zeros((batch_size, config.width, config.height, 3))\n",
    "        random.shuffle(image_filenames) \n",
    "        if ((counter+1)*batch_size>=len(image_filenames)):\n",
    "              counter = 0\n",
    "        for i in range(batch_size):\n",
    "              img = Image.open(image_filenames[counter + i]).resize((config.width, config.height))\n",
    "              #color_images[i] = np.array(img)\n",
    "              #bw_images[i] = np.array(img.convert('L'))\n",
    "              color_images[i] = np.array(img)/255  \n",
    "              bw_images[i] = np.array(img.convert('L'))/255\n",
    "        yield (bw_images, color_images)\n",
    "        counter += batch_size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test_bw_images, test_color_images) = next(my_generator(145, train_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 256, 256)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bw_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145, 256, 256, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02745098, 0.02745098, 0.02745098, ..., 0.01176471, 0.01176471,\n",
       "        0.01176471],\n",
       "       [0.02745098, 0.02745098, 0.02745098, ..., 0.01176471, 0.01176471,\n",
       "        0.01176471],\n",
       "       [0.02745098, 0.02745098, 0.02745098, ..., 0.01176471, 0.01176471,\n",
       "        0.01176471],\n",
       "       ...,\n",
       "       [0.95686275, 0.86666667, 0.81960784, ..., 0.51372549, 0.59607843,\n",
       "        0.70196078],\n",
       "       [0.78823529, 0.7372549 , 0.84313725, ..., 0.4627451 , 0.52156863,\n",
       "        0.61568627],\n",
       "       [0.61960784, 0.67058824, 0.82352941, ..., 0.39215686, 0.46666667,\n",
       "        0.57647059]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bw_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.03137255, 0.03137255, 0.03137255, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        [0.03137255, 0.03137255, 0.03137255, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        [0.03137255, 0.03137255, 0.03137255, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        ...,\n",
       "        [0.99607843, 0.96078431, 0.95294118, ..., 0.67843137, 0.75686275,\n",
       "         0.85882353],\n",
       "        [0.91764706, 0.90588235, 1.        , ..., 0.62352941, 0.6745098 ,\n",
       "         0.76078431],\n",
       "        [0.81568627, 0.89411765, 1.        , ..., 0.55686275, 0.60784314,\n",
       "         0.71372549]]),\n",
       " array([[0.02745098, 0.02745098, 0.02745098, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        [0.02745098, 0.02745098, 0.02745098, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        [0.02745098, 0.02745098, 0.02745098, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        ...,\n",
       "        [0.97254902, 0.9372549 , 0.87058824, ..., 0.45490196, 0.54901961,\n",
       "         0.66666667],\n",
       "        [0.82745098, 0.71764706, 0.80392157, ..., 0.4       , 0.47843137,\n",
       "         0.58039216],\n",
       "        [0.61176471, 0.60784314, 0.75294118, ..., 0.32941176, 0.42352941,\n",
       "         0.54117647]]),\n",
       " array([[0.01960784, 0.01960784, 0.01960784, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        [0.01960784, 0.01960784, 0.01960784, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        [0.01960784, 0.01960784, 0.01960784, ..., 0.01176471, 0.01176471,\n",
       "         0.01176471],\n",
       "        ...,\n",
       "        [0.78431373, 0.2627451 , 0.20784314, ..., 0.38823529, 0.42352941,\n",
       "         0.50588235],\n",
       "        [0.27843137, 0.39607843, 0.64705882, ..., 0.36470588, 0.37254902,\n",
       "         0.43921569],\n",
       "        [0.17254902, 0.43921569, 0.72941176, ..., 0.31372549, 0.32156863,\n",
       "         0.40392157]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_color_images[0, :, :, 0], test_color_images[0, :, :, 1], test_color_images[0, :, :, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Reshape((config.height,config.width,1), input_shape=(config.height,config.width)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))\n",
    "model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(3, (3, 3), activation='relu', padding='same'))\n",
    "#model.add(Conv2D(3, (3, 3), activation='tanh', padding='same'))\n",
    "#model.add(UpSampling2D((2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "reshape_3 (Reshape)          (None, 256, 256, 1)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 256, 256, 64)      640       \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 128, 128, 128)     73856     \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 64, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 32, 32, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 64, 64, 64)        73792     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2 (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 128, 128, 32)      18464     \n",
      "_________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2 (None, 256, 256, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 256, 256, 3)       867       \n",
      "=================================================================\n",
      "Total params: 3,892,483\n",
      "Trainable params: 3,892,483\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "(val_bw_images, val_color_images) = next(my_generator(145, val_dir))\n",
    "\n",
    "model.fit_generator( my_generator(config.batch_size, train_dir),\n",
    "                     steps_per_epoch=20,\n",
    "                     epochs=config.num_epochs, callbacks=[WandbCallback(data_type='image', predictions=16)],\n",
    "                     validation_data=(val_bw_images, val_color_images))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
